# -*- coding: utf-8 -*-
"""weapon_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OhPofzdCxUrkNr5CvqViXJyQyKn34vj
"""

import pandas as pd
import numpy as np
import os.path
import copy
import operator
import csv
import cv2
from google.colab.patches import cv2_imshow
import tensorflow as tf
import keras
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D

# Clone git repo to be able to get the data(colab)
!git clone https://github.com/siddarth-pm/weapon-classifier.git

data_csv = pd.read_csv("weapon-classifier/metadata.csv")

img = cv2.imread("/content/weapon-classifier/weapon_detection/train/images/"+data_csv.loc[0][0])
cv2_imshow(img) # Show first image

img = cv2.imread("/content/weapon-classifier/weapon_detection/train/images/"+data_csv.loc[(a, "imagefile")])
print(img.shape)

imgs = []
labels = []
with open("/content/weapon-classifier/metadata.csv") as f:
    reader = csv.reader(f)
    c = 0
    for row in reader:
      if c==0:
        c+=1
        continue
      if (os.path.isfile("/content/weapon-classifier/weapon_detection/train/images/"+row[0])):
        img = cv2.imread("/content/weapon-classifier/weapon_detection/train/images/"+row[0])
        # Only allowing data with max size (1500, 1500)--over half of
        # the data fulfills this and the data with size less than 
        # (1500, 1500) will be padded
        if(img.shape[0] <= 1500 and img.shape[1] <= 1500):
          imgs.append(img)
          labels.append(row[2])
      c+=1

padded_imgs = []
for img in imgs:
  h = img.shape[0]
  w = img.shape[1]
  # Pad all images so that each image is (1500, 1500, 3)
  padded_imgs.append(cv2.copyMakeBorder(img, int((1500 - h)/2), int((1500-h)/2) + (h%2 != 0),
                                 int((1500-w)/2), int((1500-w)/2) + (w%2 != 0), cv2.BORDER_CONSTANT, value=[0,0,0]))

# importing ResNet50
base_model = ResNet50(input_shape=(1500, 1500, 3), weights='imagenet', include_top=False)

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1000, activation='relu')(x) # add a dense layer to train on new data
predictions = Dense(9, activation='softmax')(x) # 9 classes

# this is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)

# freezing all previous resnet layers, only training the ones we just added
for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

# configure x and y data
x = np.array(padded_imgs)
print(x.shape)
y = np.array(labels)
y_one_hot = keras.utils.to_categorical(labels, 9)

model.fit(x, y_one_hot, batch_size = 16, epochs=100) # Fit model with 100 epochs, low batch size for memory constraints

# Repeating process with validation images
val_imgs = []
val_labels = []
with open("/content/weapon-classifier/metadata.csv") as f:
    reader = csv.reader(f)
    c = 0
    for row in reader:
      if c==0:
        c+=1
        continue
      if (os.path.isfile("/content/weapon-classifier/weapon_detection/val/images/"+row[0])):
        img = cv2.imread("/content/weapon-classifier/weapon_detection/val/images/"+row[0])
        if(img.shape[0] <= 1500 and img.shape[1] <= 1500):
          val_imgs.append(img)
          val_labels.append(row[2])
      c+=1

# Padding validation images
padded_val_imgs = []
for img in val_imgs:
  h = img.shape[0]
  w = img.shape[1]
  padded_val_imgs.append(cv2.copyMakeBorder(img, int((1500 - h)/2), int((1500-h)/2) + (h%2 != 0),
                                 int((1500-w)/2), int((1500-w)/2) + (w%2 != 0), cv2.BORDER_CONSTANT, value=[0,0,0]))

# configuring validation data to make it so that we can use model.evaluate
x_val = np.array(padded_val_imgs)
y_val = np.array(val_labels)
y_val_one_hot = keras.utils.to_categorical(y_val, 9)
print(y_val_one_hot.shape)

model.evaluate(x_val, y_val_one_hot)

"""The validation error is ~80 percent, roughly 10 percent lower than our training error. Once again, a case of overfitting and adjustments to the model architecture + normalizing the data can make a difference."""

predictions = model.predict(x_val)

num_to_class = {0: "AR", 1: "Bazooka", 2: "Grenade Launcher",
                3: "Handgun", 4: "Knife", 5: "Shotgun", 6: "SMG",
                7: "Sniper", 8: "Sword"}

sample_number = 110 # Which training sample
cv2_imshow(val_imgs[sample_number])
print("Weapon Type: " + num_to_class[np.argmax(predictions[sample_number])])